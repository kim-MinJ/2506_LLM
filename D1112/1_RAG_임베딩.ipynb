{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965e89d2",
   "metadata": {},
   "source": [
    "- 환경 변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33fe69b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b5807c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pcsk_3RAn2d_6bkVakuPSMkobvFRxMZLjKBcmNWLGBdiLCAH9wvaDR6xFNKjvbCh8MgG7RQSU7H'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab343ad3",
   "metadata": {},
   "source": [
    "- Pinecone 클라이언트 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f56af177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pine = Pinecone(api_key=PINECONE_API_KEY)\n",
    "# 동일 : pine = Pinecone(api_key=getenv('PINECONE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2367d4",
   "metadata": {},
   "source": [
    "- 인덱스 생성(서버리스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d883bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc13cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"wiki\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"wiki-dksb1qh.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 1536,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pine.create_index(\n",
    "  name=index_name,\n",
    "  dimension=1536,\n",
    "  metric=\"cosine\",\n",
    "  spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08179bd3",
   "metadata": {},
   "source": [
    "- 임베딩 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f04a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용할 인덱스 가져오기\n",
    "index = pine.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88ae7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "# 임베딩 모델의 차원('dimension')과 생성한 인덱스의 dimension 이 같아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5098b3d",
   "metadata": {},
   "source": [
    "- 벡터DB 에 저장할 데이터셋 가져오기\n",
    "  - 영어로 20231101 버전의 위키백과 600만개의 row 중 100개 \n",
    "  - 데이터의 크기가 크면 임베딩 시간이 오래 걸림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2145e756",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'C:/Class250615/llm_Workspace/D1112\\train-03.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pip install datasets\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train[:100]\") = 7GB 전체를 다운로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain-03.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\load.py:1397\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1392\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1393\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1394\u001b[39m )\n\u001b[32m   1396\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\load.py:913\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    890\u001b[39m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[32m    891\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    904\u001b[39m \n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path.endswith(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\load.py:527\u001b[39m, in \u001b[36mPackagedDatasetModuleFactory.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    521\u001b[39m base_path = Path(\u001b[38;5;28mself\u001b[39m.data_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).expanduser().resolve().as_posix()\n\u001b[32m    522\u001b[39m patterns = (\n\u001b[32m    523\u001b[39m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m.data_files)\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config=\u001b[38;5;28mself\u001b[39m.download_config)\n\u001b[32m    526\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m data_files = \u001b[43mDataFilesDict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _PACKAGED_DATASETS_MODULES[\u001b[38;5;28mself\u001b[39m.name]\n\u001b[32m    535\u001b[39m builder_kwargs = {\n\u001b[32m    536\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata_files\u001b[39m\u001b[33m\"\u001b[39m: data_files,\n\u001b[32m    537\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    538\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\data_files.py:705\u001b[39m, in \u001b[36mDataFilesDict.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    700\u001b[39m out = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns.items():\n\u001b[32m    702\u001b[39m     out[key] = (\n\u001b[32m    703\u001b[39m         patterns_for_key\n\u001b[32m    704\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m     )\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\data_files.py:598\u001b[39m, in \u001b[36mDataFilesList.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[32m    596\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    597\u001b[39m         data_files.extend(\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    604\u001b[39m         )\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    606\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\datasets\\data_files.py:387\u001b[39m, in \u001b[36mresolve_pattern\u001b[39m\u001b[34m(pattern, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    386\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Unable to find 'C:/Class250615/llm_Workspace/D1112\\train-03.parquet'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# pip install datasets\n",
    "# dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train[:100]\") = 7GB 전체를 다운로드\n",
    "dataset = load_dataset(\"parquet\", data_files=[\"train-03.parquet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b0c4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 156289\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# iterable 한 Dataset 타입 리턴\n",
    "print(dataset) # num_rows : 156289개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee11a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Niklas Hogner (born 29 September 1984 in Linköping, Sweden) is a Swedish figure skater. Until 2003, he competed as a singles skater, winning four Swedish junior national titles and competing at the World Junior Figure Skating Championships.\n",
      "\n",
      "He switched to pair skating, teaming up with partner Angelika Pylkina in 2003. They were the first Swedish pairs team to compete internationally since 1962. They twice placed 5th at the World Junior Championships and won three bronze medals on the Junior Grand Prix circuit. They won the bronze medal at the 2006 Nebelhorn Trophy and won the Nordic Championships. They ended their partnership in 2007.\n",
      "\n",
      "Programs \n",
      "(with Pylkina)\n",
      "\n",
      "Results\n",
      "\n",
      "Pair skating with Pylkina\n",
      "\n",
      "Single skating\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "1984 births\n",
      "Living people\n",
      "Sportspeople from Linköping\n",
      "Swedish male single skaters\n",
      "Swedish male pair skaters\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = dataset['train'][0:100] # 여전히 dict\n",
    "print(len(data)) # dict : 4개의 키 값.\n",
    "print(data['text'][0])\n",
    "# select() 는 dataset 함수\n",
    "data = dataset[\"train\"].select(range(100)) # dict 의 리스트\n",
    "print(len(data)) # list의 크기 : 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b1e542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niklas Hogner (born 29 September 1984 in Linköping, Sweden) is a Swedish figure skater. Until 2003, he competed as a singles skater, winning four Swedish junior national titles and competing at the World Junior Figure Skating Championships.\n",
      "\n",
      "He switched to pair skating, teaming up with partner Angelika Pylkina in 2003. They were the first Swedish pairs team to compete internationally since 1962. They twice placed 5th at the World Junior Championships and won three bronze medals on the Junior Grand Prix circuit. They won the bronze medal at the 2006 Nebelhorn Trophy and won the Nordic Championships. They ended their partnership in 2007.\n",
      "\n",
      "Programs \n",
      "(with Pylkina)\n",
      "\n",
      "Results\n",
      "\n",
      "Pair skating with Pylkina\n",
      "\n",
      "Single skating\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "1984 births\n",
      "Living people\n",
      "Sportspeople from Linköping\n",
      "Swedish male single skaters\n",
      "Swedish male pair skaters\n"
     ]
    }
   ],
   "source": [
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dbe4e",
   "metadata": {},
   "source": [
    "- 청크\n",
    "  - splitter 객체를 생성해서 문자열을 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33b3fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import  RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size = 400,  # 텍스트 분할 크기\n",
    "  chunk_overlap=20, # 분할할 텍스트의 중첩 크기\n",
    "  length_function=len,\n",
    "  separators = ['\\n', ' '] # 분할할 때 사용할 텍스트(단어) 구분자\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670d5bc",
   "metadata": {},
   "source": [
    "- 청킹 후 임베딩 -> 업서트(batch_size 크기만큼 수행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c868c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12\n",
      "8\n",
      "8\n",
      "4\n",
      "1\n",
      "4\n",
      "13\n",
      "1\n",
      "6\n",
      "7\n",
      "14\n",
      "6\n",
      "2\n",
      "3\n",
      "13\n",
      "8\n",
      "1\n",
      "9\n",
      "21\n",
      "22\n",
      "6\n",
      "11\n",
      "9\n",
      "7\n",
      "19\n",
      "4\n",
      "4\n",
      "13\n",
      "12\n",
      "42\n",
      "8\n",
      "14\n",
      "5\n",
      "35\n",
      "5\n",
      "6\n",
      "8\n",
      "7\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "7\n",
      "2\n",
      "7\n",
      "21\n",
      "9\n",
      "18\n",
      "3\n",
      "15\n",
      "4\n",
      "9\n",
      "3\n",
      "6\n",
      "1\n",
      "11\n",
      "4\n",
      "9\n",
      "2\n",
      "81\n",
      "6\n",
      "2\n",
      "34\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "18\n",
      "14\n",
      "73\n",
      "14\n",
      "14\n",
      "10\n",
      "2\n",
      "34\n",
      "7\n",
      "17\n",
      "7\n",
      "3\n",
      "7\n",
      "8\n",
      "4\n",
      "7\n",
      "5\n",
      "10\n",
      "23\n",
      "6\n",
      "3\n",
      "40\n",
      "6\n",
      "10\n",
      "10\n",
      "16\n",
      "3\n",
      "12\n",
      "11\n",
      "23\n",
      "9\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "batch_size = 30\n",
    "texts = []\n",
    "metas = []\n",
    "count = 0\n",
    "\n",
    "# 데이터셋의 각 샘플에 대해 반복합니다.  features: ['id', 'url', 'title', 'text'],\n",
    "for i,sample in enumerate(data):\n",
    "    full_text = sample[\"text\"] # Wikipedia 문서 텍스트 -> 청킹 후 임베딩\n",
    "    metadata = {  # key 구성은 임의로 합니다. 청킹된 데이터의 소속을 구별\n",
    "        'wiki_id': str(sample[\"id\"]),  # Wikipedia 문서 ID\n",
    "        'url': sample['url'],\n",
    "        'title': sample[\"title\"],  # Wikipedia 문서 제목\n",
    "    }\n",
    "\n",
    "\n",
    "    chunks = splitter.split_text(full_text)  # 텍스트를 청크로 분할합니다.\n",
    "    print(len(chunks))  # full_text 를 몇개로 분할 했는지 확인\n",
    "\n",
    "    # 각 청크에 대해 반복합니다.\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # 실제로 벡터db에 업서트할 record\n",
    "        record = {\n",
    "            'chunk_id': i,  # 청크 ID\n",
    "            'head_chunk': chunk[:200],  # 전체 텍스트\n",
    "            **metadata,  # 메타데이터 언패킹\n",
    "        }\n",
    "\n",
    "        texts.append(chunk)  # 청크(분할된 텍스트)를 텍스트 목록에 추가합니다.\n",
    "        metas.append(record)  # 메타데이터를 메타데이터 목록에 추가합니다.\n",
    "\n",
    "        count += 1  # 처리된 청크 수를 증가시킵니다.\n",
    "\n",
    "        # batch_size만큼의 청크를 처리 : 임베딩 -> 업서트\n",
    "        if count % batch_size == 0:\n",
    "            # Pinecone 인덱스에 청크를 추가합니다.청크 text 갯수(batch_size) 만큼 레코드 uuid 만들기\n",
    "            ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "            # 임베딩. 30개씩(batch_size).\n",
    "            embeddings = embedding.embed_documents(texts)\n",
    "            index.upsert(\n",
    "                vectors=zip(ids, embeddings, metas),\n",
    "                namespace=\"wiki-ns1\")\n",
    "            # 청크 목록과 메타데이터 목록을 비웁니다.\n",
    "            texts = []\n",
    "            metas = []\n",
    "            # 1초 대기합니다.\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
