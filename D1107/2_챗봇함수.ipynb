{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ededc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid   # ìž„ì˜ ë¬¸ìžì—´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot_function import get_menu,get_order_price,set_order\n",
    "from chatbot_responseV2  import get_first_chatbot_response,get_followup_chatbot_response,chat_with_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8283e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfAt5s72O0t8VMC0wiR14xUN9Zk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠˆí¼ ìŠˆí”„ë¦¼ í”¼ìž ì£¼ë¬¸ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ìŠˆí¼ ìŠˆí”„ë¦¼ í”¼ìžì˜ ì‚¬ì´ì¦ˆëŠ” ì–´ë–¤ ê±¸ë¡œ ì›í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ìŠ¤ëª°, ë¯¸ë””ì—„, ë¼ì§€) ê·¸ë¦¬ê³  ìˆ˜ëŸ‰ë„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498504, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=55, prompt_tokens=366, total_tokens=421, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfGJgEfhCdouOnHPDJi0YtoPWJI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='í˜„ìž¬ ë©”ë‰´ì˜ ì‚¬ì´ì¦ˆëŠ” ìŠ¤ëª°, ë¯¸ë””ì—„, ë¼ì§€ë§Œ ì œê³µë©ë‹ˆë‹¤. ì–´ëŠ ì‚¬ì´ì¦ˆë¡œ ì£¼ë¬¸í•˜ì‹œê² ì–´ìš”? ê·¸ë¦¬ê³  ìˆ˜ëŸ‰ë„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498510, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=35, prompt_tokens=434, total_tokens=469, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfNKvYke2uzFUQQLLu5Eji7C63S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠ¤ëª° ì‚¬ì´ì¦ˆ ìŠˆí¼ ìŠˆí”„ë¦¼ ëª‡ íŒ ì£¼ë¬¸í•˜ì‹œê² ì–´ìš”? ìˆ˜ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498517, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=25, prompt_tokens=478, total_tokens=503, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfQmgI737PuZMgI9BuleSzJBBuw', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ìŠ¤ëª°\",\"quantity\":3}', name='set_order'), tool_calls=None))], created=1762498520, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=30, prompt_tokens=512, total_tokens=542, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ìŠ¤ëª°\",\"quantity\":3}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ìŠ¤ëª°', 'quantity': 3}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ìŠ¤ëª°', 'quantity': 3, 'unit_price': 10000, 'sub_total': 30000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2127, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1904, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 636, in postprocess\n",
      "    self._postprocess_message_messages(cast(MessageDict, message))\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 593, in _postprocess_message_messages\n",
      "    msg = Message(**message)  # type: ignore\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 3 validation errors for Message\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.FileMessage\n",
      "  Input should be a valid dictionary or instance of FileMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "content.ComponentMessage\n",
      "  Input should be a valid dictionary or instance of ComponentMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAfgH7vlcK1rZMIipuvrhDYuGLba', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1762498536, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=370, total_tokens=380, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2127, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1904, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 636, in postprocess\n",
      "    self._postprocess_message_messages(cast(MessageDict, message))\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 593, in _postprocess_message_messages\n",
      "    msg = Message(**message)  # type: ignore\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 3 validation errors for Message\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.FileMessage\n",
      "  Input should be a valid dictionary or instance of FileMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "content.ComponentMessage\n",
      "  Input should be a valid dictionary or instance of ComponentMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAfoBHTOQebgVYU3RRs3BxONpnmb', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1762498544, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=363, total_tokens=373, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAg4tiaTxzMj0cmn5JeZ8VHzOalV', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë ˆê·¤ëŸ¬\",\"quantity\":4}', name='set_order'), tool_calls=None))], created=1762498560, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=32, prompt_tokens=579, total_tokens=611, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë ˆê·¤ëŸ¬\",\"quantity\":4}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë ˆê·¤ëŸ¬', 'quantity': 4}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë ˆê·¤ëŸ¬', 'quantity': 4, 'unit_price': 10000, 'sub_total': 40000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgGQgf1tNnSnUsLQyQmJ0W3j9a2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠˆí¼ ìŠˆí”„ë¦¼ ë ˆê·¤ëŸ¬ 4íŒì˜ ì´ ê°€ê²©ì€ 40,000ì›ìž…ë‹ˆë‹¤.\\n\\në” ì£¼ë¬¸í•˜ì‹œê±°ë‚˜ í™•ì¸í•˜ì‹¤ ì‚¬í•­ ìžˆìœ¼ì‹ ê°€ìš”?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498572, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=38, prompt_tokens=684, total_tokens=722, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgQUlbjsMu92BkrNerPrIN9bxK6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ì±—ë´‡ ê¸°ëŠ¥ê³¼ ë‹¤ë¥¸ ì§ˆë¬¸ìž…ë‹ˆë‹¤. \\n\\ní”¼ìž ì£¼ë¬¸ê³¼ ê´€ë ¨ëœ ë©”ë‰´ë‚˜ ê°€ê²© ë¬¸ì˜ê°€ ìžˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498582, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=31, prompt_tokens=734, total_tokens=765, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgbNu7dJJPZpznKPkulUUm1XSmV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ë„¤, ë©”ë‰´ì™€ ê¸°ë³¸ ê°€ê²©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n- ìŠˆí¼ ìŠˆí”„ë¦¼: 10,000ì›\\n- ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† : 11,000ì›\\n- ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨: 12,000ì›\\n- ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„: 13,000ì›\\n\\nì‚¬ì´ì¦ˆë³„ ì¶”ê°€ ë¹„ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n- ë ˆê·¤ëŸ¬: ì¶”ê°€ ë¹„ìš© ì—†ìŒ\\n- ë¯¸ë””ì—„: 2,000ì› ì¶”ê°€\\n- ë¼ì§€: 5,000ì› ì¶”ê°€\\n- íŒ¨ë°€ë¦¬: 10,000ì› ì¶”ê°€\\n\\nì£¼ë¬¸ ì›í•˜ì‹œë©´ ì¢…ë¥˜, ì‚¬ì´ì¦ˆ, ìˆ˜ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498593, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=143, prompt_tokens=778, total_tokens=921, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgtgjZVuQAWzHSoFVap9cHHqaIj', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":3}', name='set_order'), tool_calls=None))], created=1762498611, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=29, prompt_tokens=945, total_tokens=974, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":3}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'ë¼ì§€', 'quantity': 3}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'ë¼ì§€', 'quantity': 3, 'unit_price': 17000, 'sub_total': 51000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAh8TOp8J4JXdHB0sBPRAhW6gCF9', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498626, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1039, total_tokens=1049, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAhJnFi2NZftEW2lXM0Sk5OVfOxJ', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498637, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1046, total_tokens=1056, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1024)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAhMwiuaK34zxW6C3pf2zKp5Ddrp', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498640, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1053, total_tokens=1063, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1024)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ðŸ• í”¼ìž ì£¼ë¬¸ ì±—ë´‡\")\n",
    "\n",
    "    chat_output = gr.Chatbot(label=\"PizzaOrderBot\",type=\"messages\")\n",
    "    user_input = gr.Textbox(placeholder=\"ì˜ˆ: ìŠˆí¼ ìŠˆí”„ë¦¼ ì£¼ë¬¸, ì£¼ë¬¸ ì™„ë£Œ\", label=\"ë©”ì‹œì§€ ìž…ë ¥\")\n",
    "    submit = gr.Button(\"ì „ì†¡\")\n",
    "\n",
    "    # State ì»´í¬ë„ŒíŠ¸ : ì•± ì‹¤í–‰ ì¤‘ì— ê°’ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©. ë³´ì´ì§€ëŠ” ì•ŠëŠ” ì»´í¬ë„ŒíŠ¸\n",
    "    # ëŒ€í™” ë‚´ìš© ê¸°ë¡\n",
    "    chat_history = gr.State([])   # [] ëŠ” ì´ˆê¸°ê°’\n",
    "    # ì£¼ë¬¸ ë‚´ìš© ê¸°ë¡\n",
    "    orders = gr.State({\"order_id\": None, \"content\": [], \"payment\": 0})\n",
    "\n",
    "     # ì „ì†¡ ë²„íŠ¼\n",
    "    submit.click(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders], # orders ë¥¼ chat_with_bot ì „ë‹¬\n",
    "        outputs=[user_input, chat_output, orders], # chat_with_bot í•¨ìˆ˜ì— ë³€ê²½ëœ orders ë¦¬í„´\n",
    "    )\n",
    "   # ìž…ë ¥ì°½ì—ì„œ ì—”í„°ë¡œ ì‹¤í–‰ \n",
    "    user_input.submit(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders],\n",
    "        outputs=[user_input, chat_output, orders],\n",
    "    )\n",
    "\n",
    "demo.launch(inline=False,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf54723",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 6) (2160988281.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(response.model_dump_json(indent=2))'\u001b[39m\n                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 6)\n"
     ]
    }
   ],
   "source": [
    "# gpt ì—ê²Œ ë³´ë‚´ëŠ” í•¨ìˆ˜ì˜ ìš”ì²­ì„ í…ìŠ¤íŠ¸ì—ì„œ chat_history ë¡œ ë³€ê²½.\n",
    "# í…ŒìŠ¤íŠ¸ step1\n",
    "'''\n",
    "user_input ='í”¼ìž ì£¼ë¬¸. ë©”ë‰´ ë³´ì—¬ì£¼ì„¸ìš”'\n",
    "response = get_first_chatbot_response(user_input)\n",
    "print(response.model_dump_json(indent=2))'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2676ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid type for 'messages': expected an array of objects, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'invalid_type'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m user_input =\u001b[33m'\u001b[39m\u001b[33mí”¼ìž ì£¼ë¬¸. ë©”ë‰´ ë³´ì—¬ì£¼ì„¸ìš”\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mget_first_chatbot_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m fn_name = \u001b[38;5;28mgetattr\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.function_call, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      7\u001b[39m args = json.loads(response.choices[\u001b[32m0\u001b[39m].message.function_call.arguments)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py:68\u001b[39m, in \u001b[36mget_first_chatbot_response\u001b[39m\u001b[34m(chat_history)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_first_chatbot_response\u001b[39m(chat_history):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m   response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmyfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid type for 'messages': expected an array of objects, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'invalid_type'}}"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ step2\n",
    "'''\n",
    "import json\n",
    "fn_name = getattr(response.choices[0].message.function_call, \"name\", None)\n",
    "args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "print(f'ðŸ”„log fn_name: {fn_name} , args {args}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df1b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunc_response =  globals()[fn_name](**args)  \\n# # í…ŒìŠ¤íŠ¸ step4 : í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¡œ gpt ì‘ë‹µ ìš”ì²­\\nfollowup = get_followup_chatbot_response(fn_name,func_response)\\n# print(followup.model_dump_json(indent=2))\\nprint(followup.choices[0].message.content)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # í…ŒìŠ¤íŠ¸ step3 : í•¨ìˆ˜ ì‹¤í–‰ \n",
    "'''\n",
    "func_response =  globals()[fn_name](**args)  \n",
    "# # í…ŒìŠ¤íŠ¸ step4 : í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¡œ gpt ì‘ë‹µ ìš”ì²­\n",
    "followup = get_followup_chatbot_response(fn_name,func_response)\n",
    "# print(followup.model_dump_json(indent=2))\n",
    "print(followup.choices[0].message.content)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbdcbc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_chatbot_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m user_input =\u001b[33m'\u001b[39m\u001b[33mìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨ ì£¼ë¬¸ í•©ë‹ˆë‹¤.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m user_input =\u001b[33m'\u001b[39m\u001b[33më¯¸ë””ì›€ ì£¼ë¬¸í•©ë‹ˆë‹¤.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mget_chatbot_response\u001b[49m(question=user_input)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_chatbot_response' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# user_input ='ë©”ë‰´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”'\n",
    "# user_input ='ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨ ì£¼ë¬¸ í•©ë‹ˆë‹¤.'\n",
    "user_input ='ë¯¸ë””ì›€ ì£¼ë¬¸í•©ë‹ˆë‹¤.'\n",
    "response = get_chatbot_response(question=user_input)\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31724b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df54478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
