{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ededc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid   # ì„ì˜ ë¬¸ìì—´ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot_function import get_menu,get_order_price,set_order\n",
    "from chatbot_responseV2  import get_first_chatbot_response,get_followup_chatbot_response,chat_with_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8283e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfAt5s72O0t8VMC0wiR14xUN9Zk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠˆí¼ ìŠˆí”„ë¦¼ í”¼ì ì£¼ë¬¸ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ìŠˆí¼ ìŠˆí”„ë¦¼ í”¼ìì˜ ì‚¬ì´ì¦ˆëŠ” ì–´ë–¤ ê±¸ë¡œ ì›í•˜ì‹œë‚˜ìš”? (ì˜ˆ: ìŠ¤ëª°, ë¯¸ë””ì—„, ë¼ì§€) ê·¸ë¦¬ê³  ìˆ˜ëŸ‰ë„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498504, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=55, prompt_tokens=366, total_tokens=421, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfGJgEfhCdouOnHPDJi0YtoPWJI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='í˜„ì¬ ë©”ë‰´ì˜ ì‚¬ì´ì¦ˆëŠ” ìŠ¤ëª°, ë¯¸ë””ì—„, ë¼ì§€ë§Œ ì œê³µë©ë‹ˆë‹¤. ì–´ëŠ ì‚¬ì´ì¦ˆë¡œ ì£¼ë¬¸í•˜ì‹œê² ì–´ìš”? ê·¸ë¦¬ê³  ìˆ˜ëŸ‰ë„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498510, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=35, prompt_tokens=434, total_tokens=469, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfNKvYke2uzFUQQLLu5Eji7C63S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠ¤ëª° ì‚¬ì´ì¦ˆ ìŠˆí¼ ìŠˆí”„ë¦¼ ëª‡ íŒ ì£¼ë¬¸í•˜ì‹œê² ì–´ìš”? ìˆ˜ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498517, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=25, prompt_tokens=478, total_tokens=503, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAfQmgI737PuZMgI9BuleSzJBBuw', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ìŠ¤ëª°\",\"quantity\":3}', name='set_order'), tool_calls=None))], created=1762498520, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=30, prompt_tokens=512, total_tokens=542, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ìŠ¤ëª°\",\"quantity\":3}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ìŠ¤ëª°', 'quantity': 3}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ìŠ¤ëª°', 'quantity': 3, 'unit_price': 10000, 'sub_total': 30000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2127, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1904, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 636, in postprocess\n",
      "    self._postprocess_message_messages(cast(MessageDict, message))\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 593, in _postprocess_message_messages\n",
      "    msg = Message(**message)  # type: ignore\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 3 validation errors for Message\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.FileMessage\n",
      "  Input should be a valid dictionary or instance of FileMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "content.ComponentMessage\n",
      "  Input should be a valid dictionary or instance of ComponentMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAfgH7vlcK1rZMIipuvrhDYuGLba', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1762498536, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=370, total_tokens=380, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2127, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1904, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 636, in postprocess\n",
      "    self._postprocess_message_messages(cast(MessageDict, message))\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 593, in _postprocess_message_messages\n",
      "    msg = Message(**message)  # type: ignore\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 3 validation errors for Message\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.FileMessage\n",
      "  Input should be a valid dictionary or instance of FileMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "content.ComponentMessage\n",
      "  Input should be a valid dictionary or instance of ComponentMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAfoBHTOQebgVYU3RRs3BxONpnmb', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1762498544, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=363, total_tokens=373, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAg4tiaTxzMj0cmn5JeZ8VHzOalV', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë ˆê·¤ëŸ¬\",\"quantity\":4}', name='set_order'), tool_calls=None))], created=1762498560, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=32, prompt_tokens=579, total_tokens=611, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë ˆê·¤ëŸ¬\",\"quantity\":4}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë ˆê·¤ëŸ¬', 'quantity': 4}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë ˆê·¤ëŸ¬', 'quantity': 4, 'unit_price': 10000, 'sub_total': 40000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgGQgf1tNnSnUsLQyQmJ0W3j9a2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠˆí¼ ìŠˆí”„ë¦¼ ë ˆê·¤ëŸ¬ 4íŒì˜ ì´ ê°€ê²©ì€ 40,000ì›ì…ë‹ˆë‹¤.\\n\\në” ì£¼ë¬¸í•˜ì‹œê±°ë‚˜ í™•ì¸í•˜ì‹¤ ì‚¬í•­ ìˆìœ¼ì‹ ê°€ìš”?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498572, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=38, prompt_tokens=684, total_tokens=722, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgQUlbjsMu92BkrNerPrIN9bxK6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ì±—ë´‡ ê¸°ëŠ¥ê³¼ ë‹¤ë¥¸ ì§ˆë¬¸ì…ë‹ˆë‹¤. \\n\\ní”¼ì ì£¼ë¬¸ê³¼ ê´€ë ¨ëœ ë©”ë‰´ë‚˜ ê°€ê²© ë¬¸ì˜ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498582, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=31, prompt_tokens=734, total_tokens=765, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgbNu7dJJPZpznKPkulUUm1XSmV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ë„¤, ë©”ë‰´ì™€ ê¸°ë³¸ ê°€ê²©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n- ìŠˆí¼ ìŠˆí”„ë¦¼: 10,000ì›\\n- ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† : 11,000ì›\\n- ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨: 12,000ì›\\n- ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„: 13,000ì›\\n\\nì‚¬ì´ì¦ˆë³„ ì¶”ê°€ ë¹„ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n- ë ˆê·¤ëŸ¬: ì¶”ê°€ ë¹„ìš© ì—†ìŒ\\n- ë¯¸ë””ì—„: 2,000ì› ì¶”ê°€\\n- ë¼ì§€: 5,000ì› ì¶”ê°€\\n- íŒ¨ë°€ë¦¬: 10,000ì› ì¶”ê°€\\n\\nì£¼ë¬¸ ì›í•˜ì‹œë©´ ì¢…ë¥˜, ì‚¬ì´ì¦ˆ, ìˆ˜ëŸ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762498593, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=143, prompt_tokens=778, total_tokens=921, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: None\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAgtgjZVuQAWzHSoFVap9cHHqaIj', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":3}', name='set_order'), tool_calls=None))], created=1762498611, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=29, prompt_tokens=945, total_tokens=974, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":3}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'ë¼ì§€', 'quantity': 3}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'ë¼ì§€', 'quantity': 3, 'unit_price': 17000, 'sub_total': 51000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CZAh8TOp8J4JXdHB0sBPRAhW6gCF9', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498626, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1039, total_tokens=1049, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAhJnFi2NZftEW2lXM0Sk5OVfOxJ', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498637, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1046, total_tokens=1056, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1024)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CZAhMwiuaK34zxW6C3pf2zKp5Ddrp', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1762498640, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=1053, total_tokens=1063, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1024)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n",
      "log args\n",
      "â†ª: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Class250615\\llm_Workspace\\D1107\\chatbot_responseV2.py\", line 39, in chat_with_bot\n",
      "    result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
      "             ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ğŸ• í”¼ì ì£¼ë¬¸ ì±—ë´‡\")\n",
    "\n",
    "    chat_output = gr.Chatbot(label=\"PizzaOrderBot\",type=\"messages\")\n",
    "    user_input = gr.Textbox(placeholder=\"ì˜ˆ: ìŠˆí¼ ìŠˆí”„ë¦¼ ì£¼ë¬¸, ì£¼ë¬¸ ì™„ë£Œ\", label=\"ë©”ì‹œì§€ ì…ë ¥\")\n",
    "    submit = gr.Button(\"ì „ì†¡\")\n",
    "\n",
    "    # State ì»´í¬ë„ŒíŠ¸ : ì•± ì‹¤í–‰ ì¤‘ì— ê°’ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©. ë³´ì´ì§€ëŠ” ì•ŠëŠ” ì»´í¬ë„ŒíŠ¸\n",
    "    # ëŒ€í™” ë‚´ìš© ê¸°ë¡\n",
    "    chat_history = gr.State([])   # [] ëŠ” ì´ˆê¸°ê°’\n",
    "    # ì£¼ë¬¸ ë‚´ìš© ê¸°ë¡\n",
    "    orders = gr.State({\"order_id\": None, \"content\": [], \"payment\": 0})\n",
    "\n",
    "     # ì „ì†¡ ë²„íŠ¼\n",
    "    submit.click(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders], # orders ë¥¼ chat_with_bot ì „ë‹¬\n",
    "        outputs=[user_input, chat_output, orders], # chat_with_bot í•¨ìˆ˜ì— ë³€ê²½ëœ orders ë¦¬í„´\n",
    "    )\n",
    "   # ì…ë ¥ì°½ì—ì„œ ì—”í„°ë¡œ ì‹¤í–‰ \n",
    "    user_input.submit(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders],\n",
    "        outputs=[user_input, chat_output, orders],\n",
    "    )\n",
    "\n",
    "demo.launch(inline=False,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf54723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nuser_input ='í”¼ì ì£¼ë¬¸. ë©”ë‰´ ë³´ì—¬ì£¼ì„¸ìš”'\\nresponse = get_first_chatbot_response(user_input)\\nprint(response.model_dump_json(indent=2))'\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt ì—ê²Œ ë³´ë‚´ëŠ” í•¨ìˆ˜ì˜ ìš”ì²­ì„ í…ìŠ¤íŠ¸ì—ì„œ chat_history ë¡œ ë³€ê²½.\n",
    "# í…ŒìŠ¤íŠ¸ step1\n",
    "'''\n",
    "user_input ='í”¼ì ì£¼ë¬¸. ë©”ë‰´ ë³´ì—¬ì£¼ì„¸ìš”'\n",
    "response = get_first_chatbot_response(user_input)\n",
    "print(response.model_dump_json(indent=2))'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2676ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nfn_name = getattr(response.choices[0].message.function_call, \"name\", None)\\nargs = json.loads(response.choices[0].message.function_call.arguments)\\nprint(f\\'ğŸ”„log fn_name: {fn_name} , args {args}\\')\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ step2\n",
    "'''\n",
    "import json\n",
    "fn_name = getattr(response.choices[0].message.function_call, \"name\", None)\n",
    "args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "print(f'ğŸ”„log fn_name: {fn_name} , args {args}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df1b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunc_response =  globals()[fn_name](**args)  \\n# # í…ŒìŠ¤íŠ¸ step4 : í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¡œ gpt ì‘ë‹µ ìš”ì²­\\nfollowup = get_followup_chatbot_response(fn_name,func_response)\\n# print(followup.model_dump_json(indent=2))\\nprint(followup.choices[0].message.content)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # í…ŒìŠ¤íŠ¸ step3 : í•¨ìˆ˜ ì‹¤í–‰ \n",
    "'''\n",
    "func_response =  globals()[fn_name](**args)  \n",
    "# # í…ŒìŠ¤íŠ¸ step4 : í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¡œ gpt ì‘ë‹µ ìš”ì²­\n",
    "followup = get_followup_chatbot_response(fn_name,func_response)\n",
    "# print(followup.model_dump_json(indent=2))\n",
    "print(followup.choices[0].message.content)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbdcbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# user_input ='ë©”ë‰´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”'\\n# user_input ='ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨ ì£¼ë¬¸ í•©ë‹ˆë‹¤.'\\nuser_input ='ë¯¸ë””ì›€ ì£¼ë¬¸í•©ë‹ˆë‹¤.'\\nresponse = get_chatbot_response(question=user_input)\\nprint(response)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# user_input ='ë©”ë‰´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”'\n",
    "# user_input ='ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨ ì£¼ë¬¸ í•©ë‹ˆë‹¤.'\n",
    "user_input ='ë¯¸ë””ì›€ ì£¼ë¬¸í•©ë‹ˆë‹¤.'\n",
    "response = get_chatbot_response(question=user_input)\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31724b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df54478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
