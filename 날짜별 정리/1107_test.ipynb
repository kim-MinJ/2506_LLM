{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f3516f",
   "metadata": {},
   "source": [
    "ë‚´ìš© : í”¼ì ì£¼ë¬¸ì„ ë°›ëŠ” ì±—ë´‡ì„ ë§Œë“¤ê³  ëŒ€í™”í•˜ê¸° \n",
    "\n",
    "ì£¼ìš”ì  : .appendë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë‚´ìš©ì„ ì €ì¥í•˜ë„ë¡ ë§Œë“¤ê¸°\n",
    "\n",
    "â€» ì£¼ì˜ì  : í˜„ì¬ ì´ ì½”ë“œëŠ” ë¬´ì¡°ê±´ get_menu(ë©”ë‰´ë¶ˆëŸ¬ì˜¤ê¸°)ë¥¼ ì‹¤í–‰í•œ í›„ì—ì•¼ ë™ì‘í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839c8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import openai\n",
    "import requests\n",
    "import inspect # í•¨ìˆ˜ì˜ í˜•ì‹ì„ ì•Œì•„ë‚´ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "import uuid   # ì„ì˜ ë¬¸ìì—´ ìƒì„±\n",
    "load_dotenv()\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b48e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = {\n",
    "    \"ìŠˆí¼ ìŠˆí”„ë¦¼\": 10000,\n",
    "    \"ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† \": 11000,\n",
    "    \"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\": 12000,\n",
    "    \"ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„\": 13000\n",
    "}\n",
    "\n",
    "size_price = {\"ë ˆê·¤ëŸ¬\": 0, \"ë¯¸ë””ì—„\":2000, \"ë¼ì§€\": 5000, \"íŒ¨ë°€ë¦¬\": 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfdf6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ {ë©”ë‰´, ì‚¬ì´ì¦ˆ} ë„£ì„ ìˆ˜ ìˆë„ë¡ ë¯¸ë¦¬ êµ¬ì¡° ì„¤ê³„\n",
    "def get_menu() -> dict:\n",
    "    return {\"menu\":menu, \"size\":size_price}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94705aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ê°€ê²© ê³„ì‚°\n",
    "# # ì¸ìê°’ì€ ê°ê° (ë©”ë‰´ëª…, ì‚¬ì´ì¦ˆ, ìˆ˜ëŸ‰)ì„ ë”•ì…”ë„ˆë¦¬ íƒ€ì…ìœ¼ë¡œ\n",
    "# # pizza_name, pizza_size, quantityê°€ ì–´ë””ì„œ ì •í•´ì§€ëŠ”ì§€ ë³´ë ¤ë©´ myfunctions ë¡œ\n",
    "# def get_order_price(pizza_name:str, pizza_size:str=\"ë ˆê·¤ëŸ¬\",quantity:int=1)->dict:\n",
    "#     base = menu.get(pizza_name,0)    # ì—†ëŠ” í”¼ìì´ë¦„ì´ë©´ 0\n",
    "#     extra = size_price.get(pizza_size, 0)   \n",
    "#     unit_price = base + extra \n",
    "#     sub_total = unit_price * quantity\n",
    "#     return {\n",
    "#         \"pizza_name\": pizza_name,\n",
    "#         \"pizza_size\": pizza_size,\n",
    "#         \"quantity\": quantity,\n",
    "#         \"unit_price\": unit_price,    # ê¸°ë³¸ê°€ê²© + ì‚¬ì´ì¦ˆ + (í† í•‘)\n",
    "#         \"sub_total\": sub_total     # x ìˆ˜ëŸ‰\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6add42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ë§¤ í™•ì • : orders ëŠ” ì£¼ë¬¸ ë‚´ì—­ì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "def set_order(pizza_name:str, pizza_size:str=\"ë ˆê·¤ëŸ¬\",quantity:int=1, orders:dict=None)->dict:\n",
    "    if orders is None:  # ì¶”ê°€ì£¼ë¬¸ì´ë©´ None ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
    "        orders = {\"order_id\": None, \"content\":[], \"payment\":0} # ì´ˆê¸°ê°’ ì„¸íŒ…\n",
    "\n",
    "    base = menu.get(pizza_name,0)    # ì—†ëŠ” í”¼ìì´ë¦„ì´ë©´ 0\n",
    "    extra = size_price.get(pizza_size, 0)   \n",
    "    unit_price = base + extra\n",
    "    sub_total = unit_price * quantity\n",
    "\n",
    "    if not orders.get(\"order_id\"):    # ì²˜ìŒ ì£¼ë¬¸ì´ë©´ None\n",
    "        orders[\"order_id\"] = str(uuid.uuid4())   # ì£¼ë¬¸ë²ˆí˜¸ ë‚œìˆ˜ ë¬¸ìì—´ë¡œ ìƒì„±\n",
    "\n",
    "    orders['content'].append({\n",
    "        \"order\": {\"pizza_name\": pizza_name,\"pizza_size\": pizza_size, \"quantity\": quantity },\n",
    "        \"unit_price\": unit_price,    # ê¸°ë³¸ê°€ê²© + ì‚¬ì´ì¦ˆ + (í† í•‘)\n",
    "        \"sub_total\": sub_total \n",
    "        })\n",
    "\n",
    "    orders['payment'] = sum(item['sub_total'] for item in orders['content'])\n",
    "    return {\n",
    "        \"pizza_name\": pizza_name,\n",
    "        \"pizza_size\": pizza_size,\n",
    "        \"quantity\": quantity,\n",
    "        \"unit_price\": unit_price,    # ê¸°ë³¸ê°€ê²© + ì‚¬ì´ì¦ˆ + (í† í•‘)\n",
    "        \"sub_total\": sub_total     # x ìˆ˜ëŸ‰\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ae5bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTì—ê²Œ ë¨¹ì¼ í”„ë¡¬í”„íŠ¸\n",
    "instruction = \"\"\"\n",
    "ë„ˆëŠ” í”¼ì ì£¼ë¬¸ì„ ë•ëŠ” ì±—ë´‡ì´ë‹¤.\n",
    "ì¹œì ˆí•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ëŒ€í™”í•˜ë©°, ê³ ê°ì´ ì›í•˜ëŠ” í”¼ìë¥¼ ì •í™•íˆ ì£¼ë¬¸í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•œë‹¤.\n",
    "1. ê³ ê°ì˜ ì£¼ë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•œë‹¤ - ì£¼ë¬¸ ë‚´ìš©ì€ ë©”ë‰´ ì´ë¦„, ì‚¬ì´ì¦ˆ, ìˆ˜ëŸ‰ì´ë©° ê·¸ì™¸ì—ëŠ” ì—†ìŒ.\n",
    "ì£¼ì˜ : ë©”ë‰´ ëª©ë¡ì— ì—†ëŠ” ê²ƒì€ ì£¼ë¬¸ ë°›ìœ¼ë©´ ì•ˆë¨.\n",
    "2. í”¼ìì˜ ì¢…ë¥˜ì™€ ì‚¬ì´ì¦ˆëŠ” ì§€ì‹œí•œ ëŒ€ë¡œë§Œ ê°€ê²© ì•ˆë‚´ í•´ì•¼ í•¨.\n",
    "3. ì£¼ë¬¸ì„ ì§„í–‰í•˜ë©´ ì´ ê²°ì œê¸ˆì•¡ì„ ê³„ì‚°í•˜ì—¬ ì•Œë ¤ì£¼ê³  ì£¼ë¬¸ ë²ˆí˜¸ë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥í•œë‹¤.\n",
    "4. ì¶”ê°€ ì£¼ë¬¸ì´ ì—†ëŠ”ì§€ í™•ì¸í•˜ê³  ì£¼ë¬¸ì„ í™•ì •í•˜ë©´ ì£¼ë¬¸ë²ˆí˜¸ì™€ ì£¼ë¬¸ ë‚´ì—­, ê²°ì œê¸ˆì•¡ì„ í™•ì¸í•œë‹¤.\n",
    "5. ë¶ˆí•„ìš”í•œ ì¡ë‹´ì€ ìµœì†Œí™”í•˜ê³ , ì£¼ë¬¸ê³¼ ê´€ë ¨ëœ ëŒ€í™”ì— ì§‘ì¤‘í•œë‹¤.\n",
    "6. í•­ìƒ ì •ì¤‘í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¥¼ ìœ ì§€í•œë‹¤.\n",
    "ê°€ë²¼ìš´ ì¸ì‚¬ë§ê³¼ í”¼ì ì£¼ë¬¸ ì´ì™¸ì˜ ë‹¤ë¥¸ ìš”ì²­ì€ 'ì±—ë´‡ ê¸°ëŠ¥ê³¼ ë‹¤ë¥¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.' ë¼ê³  ë‹µë³€í•´.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8d67182",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunctions = [\n",
    "    {\n",
    "        \"name\": \"get_menu\",\n",
    "        \"description\": \"í”¼ì ë©”ë‰´ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_order_price\",\n",
    "        \"description\": \"ì‚¬ìš©ìì˜ ë¬¸ì˜ì— ë”°ë¼ í”¼ì ì£¼ë¬¸ì„ ìœ„í•œ ê°€ê²©ì„ ê³„ì‚°í•©ë‹ˆë‹¤..\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"pizza_name\": {\"type\": \"string\"},\n",
    "                \"pizza_size\": {\"type\": \"string\"},\n",
    "                \"quantity\": {\"type\": \"integer\"},\n",
    "            },\n",
    "            \"required\": [\"pizza_name\", \"pizza_size\", \"quantity\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"set_order\",\n",
    "        \"description\": \"í”¼ì ì£¼ë¬¸ì„ ìƒì„±í•˜ê±°ë‚˜ ì¶”ê°€í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"pizza_name\": {\"type\": \"string\"},\n",
    "                \"pizza_size\": {\"type\": \"string\"},\n",
    "                \"quantity\": {\"type\": \"integer\"},\n",
    "                \"orders\":{\"type\":\"object\"}\n",
    "            },\n",
    "            \"required\": [\"pizza_name\", \"pizza_size\", \"quantity\",\"orders\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"complete_order\",\n",
    "        \"description\": \"ì£¼ë¬¸ì„ ì™„ë£Œí•˜ê³  ê²°ì œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"orders\":{\"type\":\"object\"}\n",
    "            },\n",
    "            \"required\": [\"orders\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b207f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_chatbot_response(chat_history):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=chat_history,\n",
    "    functions=myfunctions,\n",
    "    function_call=\"auto\"  \n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ceb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followup_chatbot_response(chat_history):\n",
    "    followup_response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=chat_history,\n",
    "        functions=myfunctions\n",
    "    )\n",
    "    return followup_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(user_input,chat_history,orders):\n",
    "    if not chat_history :\n",
    "        chat_history.append({\"role\":\"system\", \"content\":instruction}) # ìœ„ì˜ í”„ë¡¬í”„íŠ¸ ë¨¹ì´ê¸°\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input}) # ì…ë ¥ê°’ ë°›ê¸°\n",
    "    response = get_first_chatbot_response(chat_history) # gptì—ê²Œ ì–´ë–¤ í•¨ìˆ˜ ì‚¬ìš©í• ì§€ ì‘ë‹µë°›ê¸°\n",
    "    print(f'log resp : {response}')\n",
    "    message = response.choices[0].message\n",
    "    print(f'log  function_call\\nâ†ª: {message.function_call}')\n",
    "    if message.function_call:\n",
    "        fn_name = message.function_call.name # í•¨ìˆ˜ëª… ì €ì¥\n",
    "        # args = json.loads(message[\"function_call\"][\"arguments\"])\n",
    "        args = json.loads(message.function_call.arguments) # ì¸ìë¥¼ dict íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•¨.\n",
    "        print(f'log args\\nâ†ª: {args}')\n",
    "        result = globals()[fn_name](**args)   # ì§€ì •í•œ í•¨ìˆ˜ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ í•¨ìˆ˜ ê°€ì ¸ì™€ ì‹¤í–‰í•˜ê¸°\n",
    "        print(f'log result\\nâ†ª: {result}')\n",
    "        # chat_history.append(message)\n",
    "        chat_history.append({\n",
    "            \"role\": \"function\",\n",
    "            \"name\": fn_name,\n",
    "            \"content\": json.dumps(result, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "        final_response =get_followup_chatbot_response(chat_history)\n",
    "        reply = final_response.choices[0].message.content\n",
    "    else:\n",
    "        reply = message.content\n",
    "    \n",
    "    # ì‚¬ìš©ìì—ê²Œ ë³´ë‚¸ ì‘ë‹µì€ assistant role\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return '', chat_history,orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b12cf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjXggpBkKmy19F0r1QajsdgMT38e', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1763109236, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=363, total_tokens=373, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjXt56QiPIXceiSCuLvzJTIGURs7', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='get_order_price'), tool_calls=None))], created=1763109249, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=31, prompt_tokens=605, total_tokens=636, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='get_order_price')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2, 'unit_price': 15000, 'sub_total': 30000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2127, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1904, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 636, in postprocess\n",
      "    self._postprocess_message_messages(cast(MessageDict, message))\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 593, in _postprocess_message_messages\n",
      "    msg = Message(**message)  # type: ignore\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 3 validation errors for Message\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.FileMessage\n",
      "  Input should be a valid dictionary or instance of FileMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n",
      "content.ComponentMessage\n",
      "  Input should be a valid dictionary or instance of ComponentMessage [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/model_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log resp : ChatCompletion(id='chatcmpl-CbjY2FH88FdPYlVFhrIPTGRkrTOq4', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='get_menu'), tool_calls=None))], created=1763109258, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=363, total_tokens=373, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='get_menu')\n",
      "log args\n",
      "â†ª: {}\n",
      "log result\n",
      "â†ª: {'menu': {'ìŠˆí¼ ìŠˆí”„ë¦¼': 10000, 'ì•„ì´ë¦¬ì‰¬ í¬í…Œì´í† ': 11000, 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨': 12000, 'ìŠˆí”„ë¦¼ ì•Œí”„ë ˆë„': 13000}, 'size': {'ë ˆê·¤ëŸ¬': 0, 'ë¯¸ë””ì—„': 2000, 'ë¼ì§€': 5000, 'íŒ¨ë°€ë¦¬': 10000}}\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjYBd8hbUCCwi1dnYbHiUkfGnWrI', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content='\"ìŠˆí¼ ìŠˆí”„ë¦¼\" í”¼ì ë¼ì§€ ì‚¬ì´ì¦ˆ 2íŒìœ¼ë¡œ ì£¼ë¬¸ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='set_order'), tool_calls=None))], created=1763109267, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=61, prompt_tokens=608, total_tokens=669, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2, 'unit_price': 15000, 'sub_total': 30000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjYJVPrPp2oLIPvLAhcDlL94q88M', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='get_order_price'), tool_calls=None))], created=1763109275, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=31, prompt_tokens=694, total_tokens=725, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠˆí¼ ìŠˆí”„ë¦¼\",\"pizza_size\":\"ë¼ì§€\",\"quantity\":2}', name='get_order_price')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠˆí¼ ìŠˆí”„ë¦¼', 'pizza_size': 'ë¼ì§€', 'quantity': 2, 'unit_price': 15000, 'sub_total': 30000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjYZ927AlCcd6eGpTnP0SFTNSJOF', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content='ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨ íŒ¨ë°€ë¦¬ ì‚¬ì´ì¦ˆ 1íŒ ì¶”ê°€ ì£¼ë¬¸ ë„ì™€ë“œë¦´ê²Œìš”. ì ì‹œë§Œìš”.', refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"íŒ¨ë°€ë¦¬\",\"quantity\":1}', name='set_order'), tool_calls=None))], created=1763109291, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=58, prompt_tokens=806, total_tokens=864, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{\"pizza_name\":\"ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨\",\"pizza_size\":\"íŒ¨ë°€ë¦¬\",\"quantity\":1}', name='set_order')\n",
      "log args\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'íŒ¨ë°€ë¦¬', 'quantity': 1}\n",
      "log result\n",
      "â†ª: {'pizza_name': 'ìŠ¤íŒŒì´ìŠ¤ ì¹˜í‚¨', 'pizza_size': 'íŒ¨ë°€ë¦¬', 'quantity': 1, 'unit_price': 22000, 'sub_total': 22000}\n",
      "log resp : ChatCompletion(id='chatcmpl-CbjYmWM6oR0r3bgU99qM1eFDs8dMn', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=FunctionCall(arguments='{}', name='complete_order'), tool_calls=None))], created=1763109304, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c2851f862', usage=CompletionUsage(completion_tokens=10, prompt_tokens=904, total_tokens=914, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "log  function_call\n",
      "â†ª: FunctionCall(arguments='{}', name='complete_order')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\C118\\miniconda3\\envs\\llmEnv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\C118\\AppData\\Local\\Temp\\ipykernel_12316\\3143341984.py\", line 13, in chat_with_bot\n",
      "    sig = inspect.signature(globals()[fn_name]) # í•¨ìˆ˜ ì¸ì ì¤‘ì—\n",
      "                           ~~~~~~~~~^^^^^^^^^\n",
      "KeyError: 'complete_order'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ğŸ• í”¼ì ì£¼ë¬¸ ì±—ë´‡\")\n",
    "\n",
    "    chat_output = gr.Chatbot(label=\"PizzaOrderBot\",type=\"messages\")\n",
    "    user_input = gr.Textbox(placeholder=\"ì˜ˆ: ìŠˆí¼ ìŠˆí”„ë¦¼ ì£¼ë¬¸, ì£¼ë¬¸ ì™„ë£Œ\", label=\"ë©”ì‹œì§€ ì…ë ¥\")\n",
    "    submit = gr.Button(\"ì „ì†¡\")\n",
    "\n",
    "    # State ì»´í¬ë„ŒíŠ¸ : ì•± ì‹¤í–‰ ì¤‘ì— ê°’ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©. ë³´ì´ì§€ëŠ” ì•ŠëŠ” ì»´í¬ë„ŒíŠ¸\n",
    "    # ëŒ€í™” ë‚´ìš© ê¸°ë¡\n",
    "    chat_history = gr.State([])   # [] ëŠ” ì´ˆê¸°ê°’\n",
    "    # ì£¼ë¬¸ ë‚´ìš© ê¸°ë¡\n",
    "    orders = gr.State({\"order_id\": None, \"content\": [], \"payment\": 0})\n",
    "\n",
    "     # ì „ì†¡ ë²„íŠ¼\n",
    "    submit.click(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders], # orders ë¥¼ chat_with_bot ì „ë‹¬\n",
    "        outputs=[user_input, chat_output, orders], # chat_with_bot í•¨ìˆ˜ì— ë³€ê²½ëœ orders ë¦¬í„´\n",
    "    )\n",
    "   # ì…ë ¥ì°½ì—ì„œ ì—”í„°ë¡œ ì‹¤í–‰ \n",
    "    user_input.submit(\n",
    "        chat_with_bot,\n",
    "        inputs=[user_input, chat_history, orders],\n",
    "        outputs=[user_input, chat_output, orders],\n",
    "    )\n",
    "\n",
    "demo.launch(inline=False,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c0fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
